# Резкость в изображениях

## 1.	Что такое резкость?

Резкость — это субъективная характеристика изображения, определяемая степенью различимости деталей на изображении. Резкость может использоваться как важный творческий инструмент для выделения текстуры. В случае фотографий соответствующая техника и пост-обработка могут помочь значительно повысить резкость результата. На воспринимаемую резкость изображения влияют два фундаментальных фактора: разрешение и чёткость.

<div align="center">
  <img src="https://github.com/nselta/lecture/blob/349f10b34f0ff6db8198b424025a10e83363bdae/img/%D0%A0%D0%B8%D1%81%D1%83%D0%BD%D0%BE%D0%BA21.jpg/">
</div>

Четкость – степень размытия изображения. Важно не путать это понятие с резкостью, так как резкость влияет на видимость границ объектов, а четкость на проработку мелких деталей. Четкость определяется величиной контрастного перехода. 

<div align="center">
  <img src="https://github.com/nselta/lecture/blob/349f10b34f0ff6db8198b424025a10e83363bdae/img/%D0%A0%D0%B8%D1%81%D1%83%D0%BD%D0%BE%D0%BA22.jpg">
</div>

На картинке ниже четко различима граница между черным и белым. На картинке 2 добавлен блюр на переход. Добавление блюра отразилось на резкости. На картинке 2 четкость выше, так как контрастный переход ниже.

<div align="center">
  <img src="https://github.com/nselta/lecture/blob/606a5ff8c1478f801b626709b83ab72992b5dea6/img/Рисунок1.png/">
	
</div>

Разрешение – величина минимальных, но еще различимых деталей на изображении, которые ограничены размером пикселя на матрице. Проще говоря, разрешение – величина, определяющая количество точек на на единицу площади. 

<div align="center">
  <img src="https://github.com/nselta/lecture/blob/606a5ff8c1478f801b626709b83ab72992b5dea6/img/Рисунок2.png/">
</div>

## 2.	Почему понижается резкость на фото?

Причин снижения резкости может быть множество. Объектив способен навестись на резкость только на определенной дистанции. Глубину резкости можно назвать зоной визуальной резкости. Это означает, что резкость может быть настолько мала, что будет едва заметна или же может быть такой, что предметы на горизонте будут четко различимы. 

На резкость в фотоаппаратуре влияют 3 фактора: 
1)	Открытие диафрагмы

	Чем больше открыта диафрагма, тем меньше глубина резкости. В современных фотоаппаратах степень открытия диафрагмы определяется автоматически. Так для пейзажей используется более закрытые диафрагмы, а для портретов более открытые.
	
	<div align="center">
  <img src="https://github.com/nselta/lecture/blob/e3a59ba1ef710ea1ce7312cb36399c00417a33cc/img/%D0%94%D0%B8%D0%B0%D1%84%D1%80%D0%B0%D0%B3%D0%BC%D0%B0.jpeg">
</div>

2)	Фокусное расстояние

	Чем больше фокусное расстояние, тем меньше глубина резкости. Если нужно размыть фон, есть смысл установить более длинное фокусное расстояние
	
	<div align="center">
  <img src="https://github.com/nselta/lecture/blob/e3a59ba1ef710ea1ce7312cb36399c00417a33cc/img/%D0%A4%D0%A0.jpeg">
</div>

3)	Расстояние между объективом и предметом съемки

	Чем ближе предмет, тем меньше глубина резкости. Например, в макросъемке отсутствует глубина резкости. Если же требуется снять удаленные объекты, есть смысл сфокусироваться на самом отдаленном предмете

## 3.	Скользящее окно

При переносе изображения на компьютер, оно подвергается действию искажающих факторов системы, в изображение вносятся линейные искажения. Обычно эти искажения заключаются в ослаблении верхних частот спектра изображения, из-за чего уменьшается резкость изображения. Следовательно, следует повысить уровень высоких частот для повышения резкости изображения, выполнить высокочастотную фильтрацию. В результате происходит подчеркивание границ объектов, улучшается различимость мелких деталей, а так же увеличение резкости частей изображения без четких контуров. 

Методов обработки изображения и повышения резкости множество. Рассмотрим наиболее простой – метод пространственной линейной обработки изображения скользящим окном небольшого размера. Это окно перемещается по изображению, и при каждом его положении формируется один отсчет выходного поля яркости.  Н основании этого формируется фильтр, с помощью которого будет повышаться резкость изображения. 

Пусть f (n) — произвольная строка исходного нерезкого изображения. Обработка изображения осуществляется ы несколько шагов. В начале осуществляется низкочастотная фильтрация, то есть дополнительное сглаживание сигнала    (n). Далее из исходного сигнала вычитается сглаженный, получая разночастотный сигнал – высокочастотное изображение  f’(n) = f(n) –   (n). Затем разностный сигнал с некоторым коэффициентом q прибавляется к исходном. Полученный результат g (n) — изображение с повышенной резкостью. В спектре этого изображения низкочастотные компоненты не изменились, то есть общий уровень яркости остался прежним, а высокочастотные усилились, то есть, подчеркнуты локальные особенности – границы, мелкие детали

На изображении: кривая 1 – исходный сигнал, кривая 2 – сглаженный сигнал, кривая 3 – разность сигналов, кривая 4 – изображение с повышенной резкостью.

<div align="center">
  <img src="https://github.com/nselta/lecture/blob/606a5ff8c1478f801b626709b83ab72992b5dea6/img/Рисунок3.png/">
</div>

Рассмотрим этот метод для двумерного случая, как в скользящем окне. Низкочастотная фильтрация, она же сглаживание, определяется усреднением значения поля яркости в окне: 
<div align="center">
  <img src="https://github.com/nselta/lecture/blob/606a5ff8c1478f801b626709b83ab72992b5dea6/img/Рисунок4.png/">
</div>

Где D – конечная область, определяющая окно. Данное выражение задает двумерную свертку сигнала с импульсной характеристикой а(k1, k2).

Однако, что бы не сделать изображение слишком резким, важно соблюдать условие:
<div align="center">
  <img src="https://github.com/nselta/lecture/blob/606a5ff8c1478f801b626709b83ab72992b5dea6/img/Рисунок5.png/">
</div>

То есть процедура сглаживания не должна изменять постоянную составляющую изображения 

Далее вычисляется высокочастотное изображение и изображение с повышенной резкостью
<div align="center">
  <img src="https://github.com/nselta/lecture/blob/606a5ff8c1478f801b626709b83ab72992b5dea6/img/Рисунок6.png/">
</div>
<div align="center">
  <img src="https://github.com/nselta/lecture/blob/606a5ff8c1478f801b626709b83ab72992b5dea6/img/Рисунок7.png/">
</div>

Где q – Коэффициент усиления высокочастотного сигнала (q>0). Если подставить в формулу 
<div align="center">
  <img src="https://github.com/nselta/lecture/blob/606a5ff8c1478f801b626709b83ab72992b5dea6/img/Рисунок8.png/">
</div>
значения, то получим:

<div align="center">
  <img src="https://github.com/nselta/lecture/blob/606a5ff8c1478f801b626709b83ab72992b5dea6/img/Рисунок9.png/">
</div>

Далее имеет смысл привести подобные члены и получить свертку:
<div align="center">
  <img src="https://github.com/nselta/lecture/blob/606a5ff8c1478f801b626709b83ab72992b5dea6/img/Рисунок10.png/">
</div>

где h(k1, k2) – импульсная характеристика КИХ-фильтра, осуществляющего подчеркивание границ:
<div align="center">
  <img src="https://github.com/nselta/lecture/blob/606a5ff8c1478f801b626709b83ab72992b5dea6/img/Рисунок11.png/">
</div>

Звучит сложно, хоть это и не так, но для простоты понимания приведу пример: 

Обычно берется небольшой размер центрированного окна (3х3, 5х5). При этом  h(k1, k2) имеет всего несколько ненулевых отсчетов. Значения этих отсчетов удобно задавать в виде маски соответствующего размера. Типичная маска размера 3х3: 
<div align="center">
  <img src="https://github.com/nselta/lecture/blob/606a5ff8c1478f801b626709b83ab72992b5dea6/img/Рисунок12.png/">
</div>

Данная маска соответствует случаю, когда сглаживание осуществляется усреднением по 5 счетам с q = 5. 
<div align="center">
  <img src="https://github.com/nselta/lecture/blob/606a5ff8c1478f801b626709b83ab72992b5dea6/img/Рисунок13.png/">
</div>

## 4.	Выделение контуров

Задача пороговой обработки – выделение одинаковых по яркости областей. Для повышения резкости используется выделение контуров.
На первый взгляд данные алгоритмы идентичны, однако пороговая обработка выделяет области, которых в рамках одного объекта может быть множество, в то время как выделение контуров определяет именно границы объекта  в зависимости от яркости. 

Задача выделения контуров состоит в построении бинарного изображения, содержащего эти очертания — графического препарата.

На рисунке ниже изображение а – пример контура, изображение б – пример идеального контура

Данная маска соответствует случаю, когда сглаживание осуществляется усреднением по 5 счетам с q = 5. 
<div align="center">
  <img src="https://github.com/nselta/lecture/blob/606a5ff8c1478f801b626709b83ab72992b5dea6/img/Рисунок14.png/">
</div>

В двумерном случае у перепада яркости появляется еще одна важная характеристика – его ориентация (угол на плоскости). На рисунке изображен локальный участок, на котором контур прямолинеен. Идеальный детектор контура должен дать бесконечно тонкую непрерывную линию по центру области изменяющейся яркости. На правой части изображения пример идеального контура
<div align="center">
  <img src="https://github.com/nselta/lecture/blob/606a5ff8c1478f801b626709b83ab72992b5dea6/img/Рисунок15.png/">
</div>

Введенное определение не гарантирует замкнутости контурных линий. В процессе выделения контура могут быть его разрывы в тех местах, где яркость меняется недостаточно быстро

<div align="center">
  <img src="https://github.com/nselta/lecture/blob/606a5ff8c1478f801b626709b83ab72992b5dea6/img/Рисунок16.png/">
</div>

Кроме того, из-за наличия шума на изображении могут ошибочно обнаруживаться контуры там, где границ объектов нет. Все это требует специальной дополнительной обработки изображений: прослеживания границ, интерполяции, обнаружения связных кривых в множестве выделенных «обломков» контурных линий

В данном определении есть еще множество подводных камней и алгоритмов, которые потянут на отдельную лекцию, но в рамках данной нам этого хватит.

Определение контуров поможет нам в повышении четкости, которая в свою очередь повысит резкость изображения. 
Перейдем к программной реализации повышения резкости.

## 5.	Программная реализация
Известно четыре основных подхода по улучшению изображения: prediction models (предсказательные модели), edge based methods (краевые методы), image statistical methods (статистические методы) и patch based (or example-based) methods (методы основанные на паттернах). Наилучшее качество дают patch based (or example-based) methods (методы основанные на паттернах).

Для реализации будем использовать язык программирования pithon

Какая фотография может называться четкой?

Та, у которой границы объектов ярко выражены. У нечетких снимков границы объектов размыты.

Как определить границы объектов на снимке?

Границы там, где видим наибольший перепад цвета.

Не зря выше было рассказано про границы

Получается, для определения четкости снимка вначале нужно определить границы объектов фотографий, потом оценить их величину, толщину, количество и т.д.
Фотография состоит из трехмерного массива чисел от 0 до 255: (ширина, высота, 3 цвета).

Границы определяются наложением фильтра как в создании глубокой нейросети: перемножением трехмерного массива на матрицы (по каждому цвету):
<div align="center">
  <img src="https://github.com/nselta/lecture/blob/606a5ff8c1478f801b626709b83ab72992b5dea6/img/Рисунок17.png/">
</div>

При перепаде цвета результирующий массив будет выдавать высокое по модулю число.

Так определим вертикальные и горизонтальные границы. Среднеарифметическое значений показывает общие границы фотографии.

Далее необходимо определить какие границы и без воздействия четкие, а какие нет. 

Для этого можно использовать следующий алгоритм: 
1.	определяем границы оригинальной 
2.	размываем оригинальное изображение
3.	определяем границы размытого снимка 
4.	считаем отношение средних арифметических размытого и оригинального изображения
5.	полученный коэффициент характеризует четкость снимка

Логика проста: у четких фотографий изменение границ будет происходить существеннее, чем у нечетких, а значит, коэффициент будет выше.

Реализация на Python:

Подключение необходимых библиотек:
```Python
import numpy as np
import matplotlib.pyplot as plt
import cv2
```
Функция определения матрицы для определения границ
```Python
  def edges(n, orient):
  		  edges = np.ones((2*n, 2*n, 3))
  		  if orient == 'vert':
          for i in range(0, 2*n):
   	      		    edges[i][n: 2*n] *= -1
   		  elif orient == 'horiz':
   		      edges[n: 2*n] *= -1
        return edges
   ```
Под параметром n задаем количество пикселей, которые включаем в оценку границ. Ориентация матрицы может быть горизонтальной или вертикальной.
Дальнейшие функции аналогичны слою глубокой нейросети:
```Python
# Apply one filter defined by parameters W and single slice
def conv_single_step(a_slice_prev, W):
    s = W * a_slice_prev
    Z = np.sum(s)
    Z = np.abs(Z)
    
    return Z
   
# Full edge filter
def conv_forward(A_prev, W, hparameters):
    m = len(A_prev)
    (f, f, n_C) = W.shape
    stride = hparameters['stride']
    pad = hparameters['pad']
    
    Z = list()
    flag = 0
    z_max = hparameters['z_max']
    
    if len(z_max) == 0:
        z_max = list()
        flag = 1
    
    for i in range(m):
        
        (x0, x1, x2) = A_prev[i].shape
        A_prev_pad = A_prev[i][ 
                            int(x0 / 4) : int(x0 * 3 / 4), 
                            int(x1 / 4) : int(x1 * 3 / 4), 
                            :]
        
        (n_H_prev, n_W_prev, n_C_prev) = A_prev_pad.shape
        n_H = int((n_H_prev - f + 2*pad) / stride) + 1
        n_W = int((n_W_prev - f + 2*pad) / stride) + 1
        z = np.zeros((n_H, n_W))
        
        a_prev_pad = A_prev_pad
        
        for h in range(n_H):
            vert_start = h * stride
            vert_end = h * stride + f
            
            for w in range(n_W):
                horiz_start = w * stride
                horiz_end = w * stride + f
                
               
                a_slice_prev = a_prev_pad[vert_start: vert_end, horiz_start: horiz_end, :]

                weights = W[:, :, :]
                z[h, w] = conv_single_step(a_slice_prev, weights)
        
        if flag == 1:
            z_max.append(np.max(z))
        Z.append(z / z_max[i])
        
    cache = (A_prev, W, hparameters)
    
    return Z, z_max, cache

# pooling
def pool_forward(A_prev, hparameters, mode = 'max'):
    m = len(A_prev)
    f = hparameters['f']
    stride = hparameters['stride']
    
    A = list()
    
    for i in range(m):
        (n_H_prev, n_W_prev) = A_prev[i].shape
        
        n_H = int(1 + (n_H_prev - f) / stride)
        n_W = int(1 + (n_W_prev - f) / stride)
        
        a = np.zeros((n_H, n_W))
        
        for h in range(n_H):
            vert_start = h * stride
            vert_end = h * stride + f
            
            for w in range(n_W):
                horiz_start = w * stride
                horiz_end = w * stride + f
                
                a_prev_slice = A_prev[i][vert_start: vert_end, horiz_start: horiz_end]

                if mode == 'max':
                    a[h, w] = np.max(a_prev_slice)
                elif mode == 'avg':
                    a[h, w] = np.mean(a_prev_slice)
                        
        A.append(a)

    cache = (A_prev, hparameters)
    
    return A, cache
    
   ```
conv_single_step — одно перемножение цветов картинки на матрицы, выявляющую границу.

conv_forward — полное определение границ на всей фотографии.

pool_forward — уменьшаем размер полученного массива.

Для анализа используем не всё изображение, а только его центральную часть, т.к. фокус фотоаппарата чаще наводится на центр. Если снимок четкий, то и центр будет четкий.

Следующая функция определяет границы объектов на снимке, используя предыдущие функции:
```Python
# main layer
def borders(images, filter_size = 1, stride = 1, pool_stride = 2, pool_size = 2, z_max = []):
    Wv = edges(filter_size, 'vert')
    hparameters = {'pad': pad, 'stride': stride, 'pool_stride': pool_stride, 'f': pool_size, 'z_max': z_max}
    Z, z_max_v, _ = conv_forward(images, Wv, hparameters)
    
    print('edge filter applied')
    
    hparameters_pool = {'stride': pool_stride, 'f': pool_size}
    Av, _ = pool_forward(Z, hparameters_pool, mode = 'max')
    
    print('vertical filter applied')
    
    Wh = edges(filter_size, 'horiz')
    hparameters = {'pad': pad, 'stride': stride, 'pool_stride': pool_stride, 'f': pool_size, 'z_max': z_max}
    Z, z_max_h, _ = conv_forward(images, Wh, hparameters)
    
    print('edge filter applied')
    
    hparameters_pool = {'stride': pool_stride, 'f': pool_size}
    Ah, _ = pool_forward(Z, hparameters_pool, mode = 'max')
    
    print('horizontal filter applied')   
    
    return [(Av[i] + Ah[i]) / 2 for i in range(len(Av))], list(map(np.max, zip(z_max_v, z_max_h)))
```

Функция определяет вертикальные границы, затем горизонтальные, и возвращает среднее арифметическое обоих массивов.

И основная функция для выдачи параметра четкости:
```Python
# calculate borders of original and blurred images
def orig_blur(images, filter_size = 1, stride = 3, pool_stride = 2, pool_size = 2, blur = 57):
    z_max = []

    img, z_max = borders(images, 
                         filter_size = filter_size, 
                         stride = stride, 
                         pool_stride = pool_stride, 
                         pool_size = pool_size
                        )
    print('original image borders is calculated')
    
    blurred_img = [cv2.GaussianBlur(x, (blur, blur), 0) for x in images]
    print('images blurred')
    
    blurred, z_max = borders(blurred_img, 
                             filter_size = filter_size, 
                             stride = stride, 
                             pool_stride = pool_stride, 
                             pool_size = pool_size, 
                             z_max = z_max
                            )
    print('blurred image borders is calculated')

    return [np.mean(orig) / np.mean(blurred) for (orig, blurred) in zip(img, blurred)], img, blurred
```
Вначале определяем границы оригинального изображения, затем размываем снимок, потом определяем границы размытой фотографии, и, наконец, считаем отношение средних арифметических границ оригинального изображения и размытого.

Функция возвращает список коэффициентов четкости, массив границ оригинального снимка и массив границ размытого.

Безусловно можно повышать резкость изображения и без выделения границ(хотя выделение границ дает более точные коэффициенты для повышения резкости). Чтобы настроить резкость изображения с помощью Python Pillow: 
1)	Прочтите изображение с помощью Image.open(). Создайте усилитель ImageEnhance.Sharpness() для изображения. 
2)	Увеличьте резкость изображения с помощью методаhance() на требуемый коэффициент.
3)	Регулируя коэффициент, вы можете сделать изображение более резким или размытым. В то время как коэффициент 1 дает исходное изображение, factor >1 увеличивает резкость изображения, а factor <1 – размывает изображение.
```Python
from PIL import Image, ImageEnhance

	im = Image.open("original-image.png")

	enhancer = ImageEnhance.Sharpness(im)

	factor = 1
	im_s_1 = enhancer.enhance(factor)
	im_s_1.save('original-image-1.png');

	factor = 0.05
	im_s_1 = enhancer.enhance(factor)
	im_s_1.save('blurred-image.png');

	factor = 2
	im_s_1 = enhancer.enhance(factor)
	im_s_1.save('sharpened-image.png');
```
Исходное изображение:
<div align="center">
  <img src="https://github.com/nselta/lecture/blob/606a5ff8c1478f801b626709b83ab72992b5dea6/img/Рисунок18.png">
</div>

Изображение с повышенной резкостью
<div align="center">
  <img src="https://github.com/nselta/lecture/blob/606a5ff8c1478f801b626709b83ab72992b5dea6/img/Рисунок19.png/">
</div>

Размытое изображение:
<div align="center">
  <img src="https://github.com/nselta/lecture/blob/606a5ff8c1478f801b626709b83ab72992b5dea6/img/Рисунок20.png/">
</div>

## Подведем итог
Резкозть - параметр изображения, отвечающий за различимость деталей на изображении.

Четкость - параметр изображения, отвечающий за степень размытия изображения.

Разрешение - параметр изображения, показывающий минимальные, но еще различимые детали изображения.
